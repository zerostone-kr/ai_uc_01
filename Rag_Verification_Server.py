
import nest_asyncio
nest_asyncio.apply()

import streamlit as st
import faiss
import numpy as np
import json
import logging
import time
from pypdf import PdfReader
from sentence_transformers import SentenceTransformer
import requests
import re
import os
from sklearn.metrics.pairwise import cosine_similarity

# -----------------------
# ë¡œê¹… ì„¤ì •
# -----------------------
logging.basicConfig(
    level=logging.INFO,
    format="[%(asctime)s] %(levelname)s: %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)

# -----------------------
# ì„¤ì •
# -----------------------
EMBEDDING_MODEL_NAME = "jhgan/ko-sbert-nli"
FAISS_INDEX_FILE = "regulation_index.faiss"
TEXT_LIST_FILE = "regulation_sentences.json"
OLLAMA_API_URL = "http://localhost:11434/api/generate"
MODEL_NAME = "my-ko-model"
FIXED_PDF_PATH = "ì¼ë°˜ì‹ ìš©ì •ë³´ê´€ë¦¬ê·œì•½(ì œ2025-4ì°¨ ì‹ ìš©ì •ë³´ì§‘ì¤‘ê´€ë¦¬ìœ„ì›íšŒ).pdf"

sentence_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
faiss_index = None
loaded_sentences = None

if os.path.exists(FAISS_INDEX_FILE):
    faiss_index = faiss.read_index(FAISS_INDEX_FILE)
    with open(TEXT_LIST_FILE, encoding="utf-8") as f:
        loaded_sentences = json.load(f)

# -----------------------
# ê·œì•½ PDF â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ
# -----------------------
def extract_text_from_pdf(pdf_path):
    logging.info("PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘...")
    start_time = time.time()
    reader = PdfReader(pdf_path)
    text = ""
    for page in reader.pages:
        page_text = page.extract_text()
        if page_text:
            text += page_text + "\n"
    logging.info(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ. ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
    return text

# -----------------------
# í…ìŠ¤íŠ¸ ì „ì²´ â†’ ë¬¸ì¥ ë¶„í• 
# -----------------------
def extract_all_sentences(text):
    logging.info("ë¬¸ì¥ ë¶„í•  ì‹œì‘...")
    start_time = time.time()
    sentences = re.split(r'[\n\r\.\?]+', text)
    result = [s.strip() for s in sentences if len(s.strip()) > 10]
    logging.info(f"ë¬¸ì¥ ë¶„í•  ì™„ë£Œ. ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
    return result

# -----------------------
# ë¬¸ì¥ â†’ ë²¡í„° ì €ì¥
# -----------------------
def build_faiss_index(sentences):
    global faiss_index, loaded_sentences
    logging.info("FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥ ì‹œì‘...")
    start_time = time.time()
    embeddings = sentence_model.encode(sentences, convert_to_numpy=True)
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)
    faiss.write_index(index, FAISS_INDEX_FILE)
    with open(TEXT_LIST_FILE, "w", encoding="utf-8") as f:
        json.dump(sentences, f, ensure_ascii=False)
    faiss_index = index
    loaded_sentences = sentences
    logging.info(f"ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ. ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
    return index, sentences

# -----------------------
# ìœ ì‚¬ ë¬¸ì¥ ê²€ìƒ‰
# -----------------------
def search_similar_sentences(query, top_k=5):
    logging.info(f"ìœ ì‚¬ ë¬¸ì¥ ê²€ìƒ‰ ì‹œì‘... ì¿¼ë¦¬: {query}")
    start_time = time.time()
    query_vec = sentence_model.encode([query]).astype("float32")
    D, I = faiss_index.search(query_vec, top_k)
    logging.info(f"ìœ ì‚¬ ë¬¸ì¥ ê²€ìƒ‰ ì™„ë£Œ. ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
    return [loaded_sentences[i] for i in I[0]]

# -----------------------
# ìœ ì‚¬ ë¬¸ì¥ ì¤‘ë³µ ì œê±°
# -----------------------
def deduplicate_sentences(sentences, threshold=0.95):
    if len(sentences) <= 1:
        return sentences
    embeddings = sentence_model.encode(sentences, convert_to_numpy=True)
    sim_matrix = cosine_similarity(embeddings)
    unique_indices = []
    for i in range(len(sentences)):
        if all(sim_matrix[i][j] < threshold for j in unique_indices):
            unique_indices.append(i)
    return [sentences[i] for i in unique_indices]

# -----------------------
# JSON ì‚¬ì „ ê²€ì¦ ë¡œì§
# -----------------------
def prevalidate_json(user_json):
    try:
        data = json.loads(user_json)
        violations = []

        if data.get("íŠ¹ìˆ˜ì±„ê¶Œì—¬ë¶€") == True:
            if not str(data.get("ë“±ë¡ì½”ë“œ", "")).startswith("7"):
                violations.append("- [ë“±ë¡ì½”ë“œ]: íŠ¹ìˆ˜ì±„ê¶Œì¸ë° ë“±ë¡ì½”ë“œê°€ 7ë¡œ ì‹œì‘í•˜ì§€ ì•ŠìŒ")

        return violations

    except Exception as e:
        return [f"âš ï¸ JSON íŒŒì‹± ì—ëŸ¬: {str(e)}"]

# -----------------------
# Ollama ë¡œì»¬ API í˜¸ì¶œ
# -----------------------
def call_ollama_with_prompt(context, user_json):
    prompt = f"""
ë‹¤ìŒì€ í•œêµ­ì˜ ì¼ë°˜ì‹ ìš©ì •ë³´ê´€ë¦¬ê·œì•½ í•µì‹¬ ìš”ì•½ì…ë‹ˆë‹¤:
{chr(10).join([f"- {line}" for line in context])}

ì•„ë˜ëŠ” ê²€í†  ëŒ€ìƒ JSONì…ë‹ˆë‹¤. ë¶„ì„ì—ë§Œ ì°¸ê³ í•˜ê³  ì¶œë ¥ì—ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

{user_json}

ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€ë‹µí•˜ì„¸ìš”:

â“ ì´ JSON ë°ì´í„°ì— ê·œì•½ ìœ„ë°˜ í•­ëª©ì´ ìˆë‹¤ë©´, ì•„ë˜ í˜•ì‹ìœ¼ë¡œë§Œ ê°„ë‹¨íˆ ì‘ì„±í•´ ì£¼ì„¸ìš”:

- [í•­ëª©ëª…]: ìœ„ë°˜ ì‚¬ìœ 

ğŸ›‘ ì ˆëŒ€ ë‹¤ìŒ ë‚´ìš©ì„ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”:
- "ë‹¤ìŒì€", "ì˜ˆì‹œ", "ì¶œë ¥ í˜•ì‹" ë“±ì˜ ì„¤ëª… ë¬¸ì¥
- JSON ë‚´ìš© ë˜ëŠ” í•­ëª©ë³„ ì„¤ëª…
- ì¶œë ¥ í˜•ì‹ ì•ˆë‚´
- ê·œì•½ ìœ„ë°˜ì´ ì—†ìœ¼ë©´ ì•„ë¬´ ê²ƒë„ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.

ë‹¨ìˆœíˆ ê·œì•½ ìœ„ë°˜ í•­ëª©ê³¼ ê·¸ ì‚¬ìœ ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""

    logging.info("ğŸ“¡ Ollama API í˜¸ì¶œ ì‹œì‘...")
    start = time.time()
    response = requests.post(OLLAMA_API_URL, json={
        "model": MODEL_NAME,
        "prompt": prompt,
        "stream": False
    })
    elapsed = time.time() - start
    logging.info(f"âœ… Ollama ì‘ë‹µ ì™„ë£Œ. ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")

    try:
        raw_output = response.json().get("response", "âš ï¸ ì‘ë‹µì— 'response' í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
    except json.JSONDecodeError:
        return f"âš ï¸ ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨:\n{response.text}"

    if isinstance(raw_output, str):
        filtered_lines = []
        for line in raw_output.splitlines():
            if any(bad_word in line for bad_word in [    "ì˜ˆì‹œ", "ì¶œë ¥ í˜•ì‹", "ë‹¤ìŒì€", "â€»", "ì„¤ëª…", "ê²°ê³¼ì…ë‹ˆë‹¤",    "ì•„ë˜ì™€ ê°™ì€ ì§ˆë¬¸", "ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€ë‹µ", "â“"]):
                continue
            if line.strip() == "":
                continue
            filtered_lines.append(line)
        
        if not filtered_lines:
            return "âœ… ê²€ì¦ ê²°ê³¼: ì´ìƒ ì—†ìŒ"
        
        return "\n".join(filtered_lines).strip()

    return raw_output

# -----------------------
# Streamlit ì¸í„°í˜ì´ìŠ¤
# -----------------------
st.set_page_config(page_title="ì‹ ìš©ë„íŒë‹¨ì •ë³´ ê²€ì¦", layout="wide")
st.title("ğŸ“„ ì¼ë°˜ì‹ ìš©ì •ë³´ê´€ë¦¬ê·œì•½ ê¸°ë°˜ ê²€ì¦ (ë¡œì»¬ Ollama API)")

# PDF ë¡œë”©ì€ ìµœì´ˆ ì‹¤í–‰ ì‹œ 1íšŒë§Œ ìˆ˜í–‰
if 'initialized' not in st.session_state:
    with st.spinner("ğŸ“˜ ê·œì•½ PDF ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤..."):
        if not os.path.exists(FIXED_PDF_PATH):
            st.error("âŒ ê³ ì •ëœ ê·œì•½ PDF íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            st.stop()
        else:
            text = extract_text_from_pdf(FIXED_PDF_PATH)
            all_sentences = extract_all_sentences(text)
            build_faiss_index(all_sentences)
            st.session_state['all_sentences'] = all_sentences
            st.session_state['initialized'] = True
            st.success(f"âœ… ê·œì•½ PDF ì²˜ë¦¬ ì™„ë£Œ. ì´ {len(all_sentences)}ê°œì˜ ë¬¸ì¥ì„ ë²¡í„°í™”í–ˆìŠµë‹ˆë‹¤.")

st.markdown("## ğŸ§ª JSON ê²€ì¦ í…ŒìŠ¤íŠ¸")

json_input = st.text_area("ê²€ì¦ ëŒ€ìƒ JSON ì…ë ¥", height=300, value='''{
  "ì‹ ìš©ì •ë³´ìœ í˜•": "ì‹ ìš©ë„íŒë‹¨ì •ë³´",
  "ë“±ë¡ê¸°ê´€": "êµ­ë¯¼ì€í–‰",
  "ë“±ë¡ì¼ì": "2025-03-10",
  "ì‹ë³„ì •ë³´": {
    "ì„±ëª…": "í™ê¸¸ë™",
    "ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸": "901010-1234567"
  },
  "ì •ë³´ìœ í˜•": "ì—°ì²´ì •ë³´",
  "ë“±ë¡ì½”ë“œ": "0101",
  "ì—°ì²´ê¸°ì‚°ì¼": "2025-01-01",
  "ë“±ë¡ì‚¬ìœ ë°œìƒì¼": "2025-03-03",
  "ë“±ë¡ê¸ˆì•¡": 1500000,
  "ì—°ì²´ê¸ˆì•¡": 1500000,
  "íŠ¹ìˆ˜ì±„ê¶Œì—¬ë¶€": true,
  "ê´€ë ¨ì¸ì—¬ë¶€": false,
  "ë³´ì „ì²˜ë¶„ë“±ì¡´ì¬ì—¬ë¶€": false,
  "í•´ì œì‚¬ìœ ": null,
  "ì†Œì†¡ì¤‘ì—¬ë¶€": false,
  "ë¹„ê³ ": "ì‹ ìš©ì¹´ë“œ ì²­êµ¬ëŒ€ê¸ˆ ë¯¸ê²°ì œì— ë”°ë¥¸ ì—°ì²´"
}''')

query = st.text_input("ì§ˆë¬¸ ìš”ì•½ (ì˜ˆ: ì—°ì²´ì •ë³´ ë“±ë¡ ê¸°ì¤€)", "ì—°ì²´ì •ë³´ ë“±ë¡ ì¡°ê±´ê³¼ íŠ¹ìˆ˜ì±„ê¶Œ ë“±ë¡ ê¸°ì¤€ ìœ„ë°˜ ì—¬ë¶€ ê²€í† ")

if st.button("ğŸš€ ê²€ì¦ ìš”ì²­"):
    with st.spinner("ê²€ì¦ ì¤‘ì…ë‹ˆë‹¤..."):
        precheck = prevalidate_json(json_input)
        top_sentences = deduplicate_sentences(search_similar_sentences(query, top_k=10))
        result = call_ollama_with_prompt(top_sentences, json_input)

        # âœ… ì‚¬ì „ê²€ì¦ ë° LLM ì‘ë‹µ í†µí•© ì²˜ë¦¬ (ì´ìƒ ì—†ìŒ ì¶œë ¥ ì™„ì „ ì°¨ë‹¨ ì¡°ê±´ í¬í•¨)
        final_lines = []

        if precheck:
            final_lines.extend(precheck)

        if result and result.strip() and result.strip() != "âœ… ê²€ì¦ ê²°ê³¼: ì´ìƒ ì—†ìŒ":
            final_lines.append(result.strip())

        if not final_lines:
            final_lines.append("âœ… ê²€ì¦ ê²°ê³¼: ì´ìƒ ì—†ìŒ")

        st.markdown("### âœ… ê²€ì¦ ê²°ê³¼\n```markdown\n" + "\n".join(final_lines) + "\n```") 
