import nest_asyncio
nest_asyncio.apply()

import streamlit as st
import faiss
import numpy as np
import json
import logging
import time
from pypdf import PdfReader
from sentence_transformers import SentenceTransformer
import requests
import re
import os
from sklearn.metrics.pairwise import cosine_similarity

# -----------------------
# ë¡œê¹… ì„¤ì •
# -----------------------
logging.basicConfig(
    level=logging.INFO,
    format="[%(asctime)s] %(levelname)s: %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)

# -----------------------
# ì„¤ì •
# -----------------------
EMBEDDING_MODEL_NAME = "jhgan/ko-sbert-nli"
FAISS_INDEX_FILE = "regulation_index.faiss"
TEXT_LIST_FILE = "regulation_sentences.json"
OLLAMA_API_URL = "http://localhost:11434/api/generate"
MODEL_NAME = "my-ko-model"
FIXED_PDF_PATH = "ì¼ë°˜ì‹ ìš©ì •ë³´ê´€ë¦¬ê·œì•½(ì œ2025-4ì°¨ ì‹ ìš©ì •ë³´ì§‘ì¤‘ê´€ë¦¬ìœ„ì›íšŒ).pdf"

sentence_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
faiss_index = None
loaded_sentences = None

if os.path.exists(FAISS_INDEX_FILE):
    faiss_index = faiss.read_index(FAISS_INDEX_FILE)
    with open(TEXT_LIST_FILE, encoding="utf-8") as f:
        loaded_sentences = json.load(f)

# -----------------------
# í…ìŠ¤íŠ¸ ë° ë²¡í„° ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# -----------------------
def extract_text_from_pdf(pdf_path):
    reader = PdfReader(pdf_path)
    return "\n".join([page.extract_text() for page in reader.pages if page.extract_text()])

def extract_all_sentences(text):
    sentences = re.split(r'[\n\r\.\?]+', text)
    return [s.strip() for s in sentences if len(s.strip()) > 10]

def build_faiss_index(sentences):
    global faiss_index, loaded_sentences
    embeddings = sentence_model.encode(sentences, convert_to_numpy=True)
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)
    faiss.write_index(index, FAISS_INDEX_FILE)
    with open(TEXT_LIST_FILE, "w", encoding="utf-8") as f:
        json.dump(sentences, f, ensure_ascii=False)
    faiss_index = index
    loaded_sentences = sentences

def search_similar_sentences(query, top_k=5):
    query_vec = sentence_model.encode([query]).astype("float32")
    D, I = faiss_index.search(query_vec, top_k)
    return [loaded_sentences[i] for i in I[0]]

def deduplicate_sentences(sentences, threshold=0.95):
    if len(sentences) <= 1:
        return sentences
    embeddings = sentence_model.encode(sentences, convert_to_numpy=True)
    sim_matrix = cosine_similarity(embeddings)
    unique_indices = []
    for i in range(len(sentences)):
        if all(sim_matrix[i][j] < threshold for j in unique_indices):
            unique_indices.append(i)
    return [sentences[i] for i in unique_indices]

# -----------------------
# Ollama API í˜¸ì¶œ
# -----------------------
def call_ollama_with_prompt(context, user_json):
    prompt = f"""
ë‹¤ìŒì€ í•œêµ­ì˜ ì¼ë°˜ì‹ ìš©ì •ë³´ê´€ë¦¬ê·œì•½ í•µì‹¬ ìš”ì•½ìž…ë‹ˆë‹¤:
{chr(10).join([f"- {line}" for line in context])}

ê²€í†  ëŒ€ìƒ JSONì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ë¶„ì„ì—ë§Œ ì°¸ê³ í•˜ê³  ì¶œë ¥ì—ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”:

{user_json}

ì´ JSON ë°ì´í„°ì— ê·œì•½ ìœ„ë°˜ì´ ìžˆë‹¤ë©´ ì•„ëž˜ í˜•ì‹ìœ¼ë¡œ ê°„ë‹¨ížˆ ìž‘ì„±í•˜ì„¸ìš”:
- [í•­ëª©ëª…]: ìœ„ë°˜ ì‚¬ìœ 

ðŸ›‘ ì¶œë ¥ ì‹œ ì•„ëž˜ ë‚´ìš©ì„ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”:
- \"ë‹¤ìŒì€\", \"ì˜ˆì‹œ\", \"ì¶œë ¥ í˜•ì‹\" ë“±ì˜ ì„¤ëª…
- JSON ë³¸ë¬¸ ë˜ëŠ” í•­ëª© ì„¤ëª…
- í˜•ì‹ ì•ˆë‚´ ë¬¸êµ¬
- ì•„ë¬´ ìœ„ë°˜ë„ ì—†ì„ ê²½ìš° ì•„ë¬´ ê²ƒë„ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.
"""

    response = requests.post(OLLAMA_API_URL, json={
        "model": MODEL_NAME,
        "prompt": prompt,
        "stream": False
    })

    try:
        raw_output = response.json().get("response", "âš ï¸ ì‘ë‹µì— 'response' í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
    except json.JSONDecodeError:
        return f"âš ï¸ ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨:\n{response.text}"

    if isinstance(raw_output, str):
        filtered_lines = []
        for line in raw_output.splitlines():
            if any(bad_word in line for bad_word in [
                "ì˜ˆì‹œ", "ì¶œë ¥ í˜•ì‹", "ë‹¤ìŒì€", "â€»", "ì„¤ëª…", "ê²°ê³¼ìž…ë‹ˆë‹¤",
                "ì•„ëž˜ì™€ ê°™ì€ ì§ˆë¬¸", "ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€ë‹µ", "â“"
            ]):
                continue
            if line.strip():
                filtered_lines.append(line.strip())

        return "\n".join(filtered_lines).strip()

    return raw_output

# -----------------------
# Streamlit UI
# -----------------------
st.set_page_config(page_title="ì‹ ìš©ë„íŒë‹¨ì •ë³´ ê²€ì¦", layout="wide")
st.title("ðŸ“„ ì¼ë°˜ì‹ ìš©ì •ë³´ê´€ë¦¬ê·œì•½ ê¸°ë°˜ ê²€ì¦ (ë¡œì»¬ Ollama API)")

# âœ… ìµœì´ˆ 1íšŒ PDF ì²˜ë¦¬
if 'initialized' not in st.session_state:
    with st.spinner("ðŸ“˜ ê·œì•½ PDF ì²˜ë¦¬ ì¤‘ìž…ë‹ˆë‹¤..."):
        if not os.path.exists(FIXED_PDF_PATH):
            st.error("âŒ ê³ ì •ëœ ê·œì•½ PDF íŒŒì¼ì´ ì¡´ìž¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            st.stop()
        else:
            text = extract_text_from_pdf(FIXED_PDF_PATH)
            all_sentences = extract_all_sentences(text)
            build_faiss_index(all_sentences)
            st.session_state['all_sentences'] = all_sentences
            st.session_state['initialized'] = True
            st.success(f"âœ… ê·œì•½ PDF ì²˜ë¦¬ ì™„ë£Œ. ì´ {len(all_sentences)}ê°œì˜ ë¬¸ìž¥ì„ ë²¡í„°í™”í–ˆìŠµë‹ˆë‹¤.")

# âœ… ê²€ì¦ ìž…ë ¥ UI
st.markdown("## ðŸ§ª JSON ê²€ì¦ í…ŒìŠ¤íŠ¸")

json_input = st.text_area("ê²€ì¦ ëŒ€ìƒ JSON ìž…ë ¥", height=300, value='''{
  "ì‹ ìš©ì •ë³´ìœ í˜•": "ì‹ ìš©ë„íŒë‹¨ì •ë³´",
  "ë“±ë¡ê¸°ê´€": "êµ­ë¯¼ì€í–‰",
  "ë“±ë¡ì¼ìž": "2025-03-10",
  "ì‹ë³„ì •ë³´": {
    "ì„±ëª…": "í™ê¸¸ë™",
    "ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸": "901010-1234567"
  },
  "ì •ë³´ìœ í˜•": "ì—°ì²´ì •ë³´",
  "ë“±ë¡ì½”ë“œ": "0101",
  "ì—°ì²´ê¸°ì‚°ì¼": "2025-01-01",
  "ë“±ë¡ì‚¬ìœ ë°œìƒì¼": "2025-03-03",
  "ë“±ë¡ê¸ˆì•¡": 1500000,
  "ì—°ì²´ê¸ˆì•¡": 1500000,
  "íŠ¹ìˆ˜ì±„ê¶Œì—¬ë¶€": true,
  "ê´€ë ¨ì¸ì—¬ë¶€": false,
  "ë³´ì „ì²˜ë¶„ë“±ì¡´ìž¬ì—¬ë¶€": false,
  "í•´ì œì‚¬ìœ ": null,
  "ì†Œì†¡ì¤‘ì—¬ë¶€": false,
  "ë¹„ê³ ": "ì‹ ìš©ì¹´ë“œ ì²­êµ¬ëŒ€ê¸ˆ ë¯¸ê²°ì œì— ë”°ë¥¸ ì—°ì²´"
}''')

if st.button("ðŸš€ ê²€ì¦ ìš”ì²­"):
    with st.spinner("ê²€ì¦ ì¤‘ìž…ë‹ˆë‹¤..."):
        try:
            json_data = json.loads(json_input)
            queries = [f"{k}: {v}" for k, v in json_data.items() if not isinstance(v, dict)]
            if 'ì‹ë³„ì •ë³´' in json_data:
                queries.append(f"ì„±ëª…: {json_data['ì‹ë³„ì •ë³´'].get('ì„±ëª…', '')}")
                queries.append(f"ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸: {json_data['ì‹ë³„ì •ë³´'].get('ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸', '')}")
        except Exception as e:
            st.error(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
            st.stop()

        retrieved = []
        for q in queries:
            retrieved.extend(search_similar_sentences(q, top_k=3))
        top_sentences = deduplicate_sentences(retrieved)

        result = call_ollama_with_prompt(top_sentences, json_input)

        final_lines = []
        if result and result.strip() and result.strip() != "âœ… ê²€ì¦ ê²°ê³¼: ì´ìƒ ì—†ìŒ":
            final_lines.append(result.strip())

        if not final_lines:
            final_lines.append("âœ… ê²€ì¦ ê²°ê³¼: ì´ìƒ ì—†ìŒ")

        st.markdown("### âœ… ê²€ì¦ ê²°ê³¼\n```markdown\n" + "\n".join(final_lines) + "\n```")
